{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APIM ‚ù§Ô∏è Azure AI Foundry\n",
    "\n",
    "## Azure AI Foundry OpenAI Compatibility lab\n",
    "![flow](../../images/azure-ai-foundry-openai-compatibility.gif)\n",
    "\n",
    "Playground to configure [Azure AI Foundry](https://azure.microsoft.com/en-us/products/ai-foundry) with Azure API Management for optimal OpenAI API compatibility with VS Code extensions like [Cline](https://marketplace.visualstudio.com/items?itemName=saoudrizwan.claude-dev), LangChain, and other OpenAI-compatible clients. This lab demonstrates how to set up endpoints, configure model switching through deployment names, and provide seamless integration for development tools.\n",
    "\n",
    "### Key Features\n",
    "- ‚úÖ **OpenAI API Compatibility**: Full compatibility with OpenAI SDK and extensions\n",
    "- üîÑ **Model Switching**: Switch models by changing deployment names in the endpoint\n",
    "- üîß **VS Code Extension Ready**: Optimized for Cline and other AI coding assistants\n",
    "- üõ°Ô∏è **Secure**: Uses managed identity and Azure API Management for security\n",
    "- üìä **Monitoring**: Built-in token tracking and usage analytics\n",
    "\n",
    "[View policy configuration](policy.xml)\n",
    "\n",
    "### TOC\n",
    "- [0Ô∏è‚É£ Initialize notebook variables](#0)\n",
    "- [1Ô∏è‚É£ Create deployment using ü¶æ Bicep](#1)\n",
    "- [2Ô∏è‚É£ Get the deployment outputs](#2)\n",
    "- [3Ô∏è‚É£ Install Python packages](#3)\n",
    "- [4Ô∏è‚É£ Configure VS Code Extensions](#4)\n",
    "- [üß™ Test OpenAI compatibility](#test)\n",
    "- [üìä Monitor usage](#monitor)\n",
    "- [üóëÔ∏è Clean up resources](#clean)\n",
    "\n",
    "### Prerequisites\n",
    "- [Python 3.12 or later version](https://www.python.org/) installed\n",
    "- [VS Code](https://code.visualstudio.com/) installed with the [Jupyter notebook extension](https://marketplace.visualstudio.com/items?itemName=ms-toolsai.jupyter) enabled\n",
    "- [Azure CLI](https://learn.microsoft.com/cli/azure/install-azure-cli) installed\n",
    "- [An Azure Subscription](https://azure.microsoft.com/free/) with Contributor permissions\n",
    "- [Sign in to Azure with Azure CLI](https://learn.microsoft.com/cli/azure/authenticate-azure-cli-interactively)\n",
    "- Azure AI Foundry project with deployed models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "### 0Ô∏è‚É£ Initialize notebook variables\n",
    "\n",
    "- Resources will be suffixed by a unique string based on your subscription id.\n",
    "- Adjust the location parameters according your preferences and on the [product availability by Azure region.](https://azure.microsoft.com/explore/global-infrastructure/products-by-region/?cdn=disable&products=cognitive-services,api-management) \n",
    "- Configure your Azure AI Foundry project details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "sys.path.insert(1, '../../shared')  # add the shared directory to the Python path\n",
    "from utils import get_unique_identifier\n",
    "\n",
    "# Set your Azure subscription ID and resource group name\n",
    "subscription_id = !az account show --query id --output tsv\n",
    "subscription_id = subscription_id[0]\n",
    "\n",
    "# Azure location and resource group name\n",
    "location = \"eastus\"\n",
    "resource_group_name = f\"rg-ai-foundry-openai-compatibility-{get_unique_identifier()}\"\n",
    "\n",
    "# Azure AI Foundry configuration\n",
    "# Replace these with your actual Azure AI Foundry project details\n",
    "ai_foundry_project_name = \"your-ai-foundry-project\"  # Update this\n",
    "ai_foundry_resource_group = \"your-ai-foundry-rg\"     # Update this\n",
    "ai_foundry_subscription_id = subscription_id\n",
    "\n",
    "# Available models in your Azure AI Foundry project\n",
    "available_models = [\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\", \n",
    "    \"gpt-35-turbo\",\n",
    "    \"gpt-4\"\n",
    "]\n",
    "\n",
    "# Default model for testing\n",
    "default_model = \"gpt-4o\"\n",
    "\n",
    "# APIM Configuration\n",
    "apim_sku = \"Consumption\"  # or \"Developer\", \"Standard\", \"Premium\"\n",
    "\n",
    "print(f\"üîç Subscription ID: {subscription_id}\")\n",
    "print(f\"üìç Location: {location}\")\n",
    "print(f\"üì¶ Resource Group: {resource_group_name}\")\n",
    "print(f\"ü§ñ AI Foundry Project: {ai_foundry_project_name}\")\n",
    "print(f\"üéØ Default Model: {default_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "### 1Ô∏è‚É£ Create deployment using ü¶æ Bicep\n",
    "\n",
    "This step will create:\n",
    "- Azure API Management instance\n",
    "- Azure AI Foundry backend configuration\n",
    "- OpenAI-compatible API with policies\n",
    "- Subscriptions for VS Code extensions\n",
    "- Application Insights for monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create resource group\n",
    "!az group create --name {resource_group_name} --location {location}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the Bicep template\n",
    "deployment_name = f\"ai-foundry-openai-compatibility-{get_unique_identifier()}\"\n",
    "\n",
    "deployment_command = f\"\"\"az deployment group create \\\n",
    "  --resource-group {resource_group_name} \\\n",
    "  --template-file main.bicep \\\n",
    "  --name {deployment_name} \\\n",
    "  --parameters location={location} \\\n",
    "  --parameters apimSku={apim_sku} \\\n",
    "  --parameters aiFoundryConfig='{{\"projectName\":\"{ai_foundry_project_name}\",\"resourceGroupName\":\"{ai_foundry_resource_group}\",\"subscriptionId\":\"{ai_foundry_subscription_id}\"}}'\"\"\"\n",
    "\n",
    "print(f\"üöÄ Deploying infrastructure...\\n\")\n",
    "print(f\"üìã Command: {deployment_command}\\n\")\n",
    "\n",
    "!{deployment_command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "### 2Ô∏è‚É£ Get the deployment outputs\n",
    "\n",
    "Extract the necessary information from the deployment to configure VS Code extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get deployment outputs\n",
    "outputs = !az deployment group show --resource-group {resource_group_name} --name {deployment_name} --query properties.outputs --output json\n",
    "outputs_json = json.loads(outputs[0])\n",
    "\n",
    "# Extract values\n",
    "apim_service_name = outputs_json['apimServiceName']['value']\n",
    "apim_gateway_url = outputs_json['apimResourceGatewayURL']['value']\n",
    "subscription_keys = outputs_json['apimSubscriptionKeys']['value']\n",
    "openai_endpoints = outputs_json['openaiCompatibleEndpoints']['value']\n",
    "vscode_configs = outputs_json['vsCodeExtensionConfigs']['value']\n",
    "\n",
    "# Get primary subscription key for Cline extension\n",
    "cline_subscription_key = subscription_keys[0]['primaryKey']\n",
    "\n",
    "print(f\"‚úÖ Deployment completed successfully!\\n\")\n",
    "print(f\"üîó APIM Gateway URL: {apim_gateway_url}\")\n",
    "print(f\"üîë Subscription Key (Cline): {cline_subscription_key[:8]}...\")\n",
    "print(f\"üéØ Base API URL: {openai_endpoints['baseUrl']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "### 3Ô∏è‚É£ Install Python packages\n",
    "\n",
    "Install required packages for testing OpenAI compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4'></a>\n",
    "### 4Ô∏è‚É£ Configure VS Code Extensions\n",
    "\n",
    "Here are the configuration examples for popular VS Code extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß VS Code Extension Configurations\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Cline Extension Configuration\n",
    "print(\"\\nüì± Cline Extension (claude-dev)\")\n",
    "print(\"-\" * 30)\n",
    "cline_config = vscode_configs['clineExtension']['settings']\n",
    "print(\"Add these settings to your VS Code settings.json:\")\n",
    "print(json.dumps(cline_config, indent=2))\n",
    "\n",
    "# Generic OpenAI Configuration\n",
    "print(\"\\nüîå Generic OpenAI-Compatible Extensions\")\n",
    "print(\"-\" * 40)\n",
    "generic_config = vscode_configs['genericOpenAI']\n",
    "print(f\"Base URL: {generic_config['endpoint']}\")\n",
    "print(f\"API Key: {generic_config['apiKey'][:8]}...\")\n",
    "print(f\"Model: {generic_config['model']}\")\n",
    "\n",
    "# Model switching examples\n",
    "print(\"\\nüîÑ Model Switching Examples\")\n",
    "print(\"-\" * 25)\n",
    "for model in available_models:\n",
    "    endpoint = openai_endpoints['deploymentUrlTemplate'].replace('{deployment-name}', model)\n",
    "    print(f\"Model '{model}': {endpoint}\")\n",
    "\n",
    "print(\"\\nüí° Tips:\")\n",
    "print(\"- To switch models, change the deployment name in the endpoint URL\")\n",
    "print(\"- Use the base URL for extensions that send model name in request body\")\n",
    "print(\"- Use deployment-specific URLs for extensions that specify model via endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test'></a>\n",
    "### üß™ Test OpenAI compatibility\n",
    "\n",
    "Test the API with both OpenAI SDK and direct HTTP requests to ensure compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with OpenAI SDK\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client with our Azure AI Foundry endpoint\n",
    "client = OpenAI(\n",
    "    base_url=openai_endpoints['baseUrl'],\n",
    "    api_key=cline_subscription_key\n",
    ")\n",
    "\n",
    "print(\"üß™ Testing OpenAI SDK compatibility...\\n\")\n",
    "\n",
    "# Test messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds concisely.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Azure AI Foundry? Please respond in 2-3 sentences.\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Make the API call\n",
    "    response = client.chat.completions.create(\n",
    "        model=default_model,\n",
    "        messages=messages,\n",
    "        max_tokens=150,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    response_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Success! Response time: {response_time:.2f} seconds\")\n",
    "    print(f\"üìù Response: {response.choices[0].message.content}\")\n",
    "    \n",
    "    if response.usage:\n",
    "        print(f\"üìä Token usage:\")\n",
    "        print(f\"   - Total tokens: {response.usage.total_tokens}\")\n",
    "        print(f\"   - Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "        print(f\"   - Completion tokens: {response.usage.completion_tokens}\")\n",
    "\nexcept Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test deployment-specific endpoint\n",
    "import requests\n",
    "\n",
    "print(\"üß™ Testing deployment-specific endpoint...\\n\")\n",
    "\n",
    "# Test with deployment-specific URL\n",
    "deployment_url = openai_endpoints['deploymentUrlTemplate'].replace('{deployment-name}', default_model)\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Ocp-Apim-Subscription-Key': cline_subscription_key\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    'messages': [\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': 'Hello! Can you confirm this API is working?'}\n",
    "    ],\n",
    "    'max_tokens': 50,\n",
    "    'temperature': 0.5\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(deployment_url, json=payload, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        print(f\"‚úÖ Deployment endpoint test successful!\")\n",
    "        print(f\"üìù Response: {result['choices'][0]['message']['content']}\")\n",
    "        print(f\"üîó Endpoint: {deployment_url}\")\n",
    "    else:\n",
    "        print(f\"‚ùå HTTP {response.status_code}: {response.text}\")\n",
    "\nexcept Exception as e:\n",
    "    print(f\"‚ùå Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model switching\n",
    "print(\"üîÑ Testing model switching...\\n\")\n",
    "\n",
    "for model in available_models[:2]:  # Test first 2 models to save tokens\n",
    "    print(f\"Testing model: {model}\")\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": f\"Hello! Which model are you? Just state your model name.\"}\n",
    "            ],\n",
    "            max_tokens=30\n",
    "        )\n",
    "        \n",
    "        print(f\"  ‚úÖ {model}: {response.choices[0].message.content}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {model}: {str(e)}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='monitor'></a>\n",
    "### üìä Monitor usage\n",
    "\n",
    "Check the Application Insights for usage metrics and logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Application Insights information\n",
    "app_insights_name = f\"appi-{get_unique_identifier()}\"\n",
    "\n",
    "print(\"üìä Monitoring Information\\n\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"üîç Application Insights: {app_insights_name}\")\n",
    "print(f\"üìà Resource Group: {resource_group_name}\")\n",
    "print(f\"üåê Portal Link: https://portal.azure.com/#@/resource/subscriptions/{subscription_id}/resourceGroups/{resource_group_name}/providers/Microsoft.Insights/components/{app_insights_name}\")\n",
    "\n",
    "print(\"\\nüìã Available Metrics:\")\n",
    "print(\"- Token usage per subscription\")\n",
    "print(\"- Request latency\")\n",
    "print(\"- Error rates\")\n",
    "print(\"- Model usage distribution\")\n",
    "print(\"- Client IP tracking\")\n",
    "\n",
    "print(\"\\nüîç KQL Queries to try in Application Insights:\")\n",
    "print(\"\\n1. Token usage by model:\")\n",
    "print('customMetrics | where name == \"llm.usage.completion_tokens\" | summarize sum(value) by tostring(customDimensions.Model)')\n",
    "\n",
    "print(\"\\n2. Request count by subscription:\")\n",
    "print('requests | summarize count() by tostring(customDimensions[\"Subscription ID\"])')\n",
    "\n",
    "print(\"\\n3. Average response time:\")\n",
    "print('requests | summarize avg(duration) by bin(timestamp, 5m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Next Steps\n",
    "\n",
    "Your Azure AI Foundry OpenAI-compatible API is now ready! Here's what you can do next:\n",
    "\n",
    "#### üîß Configure VS Code Extensions\n",
    "1. **Cline Extension**: Copy the settings from above to your VS Code settings.json\n",
    "2. **Other Extensions**: Use the provided endpoint URLs and API key\n",
    "3. **Test**: Try creating a chat completion in your extension\n",
    "\n",
    "#### üîÑ Model Management\n",
    "- Switch models by changing the deployment name in the endpoint URL\n",
    "- Add new models to your Azure AI Foundry project\n",
    "- Update the `available_models` list in this notebook\n",
    "\n",
    "#### üìä Monitoring\n",
    "- Check Application Insights for usage metrics\n",
    "- Set up alerts for high token usage\n",
    "- Monitor response times and error rates\n",
    "\n",
    "#### üõ°Ô∏è Security\n",
    "- Rotate subscription keys regularly\n",
    "- Configure IP restrictions if needed\n",
    "- Set up rate limiting policies\n",
    "\n",
    "#### üöÄ Production Considerations\n",
    "- Upgrade to a higher APIM tier for production workloads\n",
    "- Configure custom domains\n",
    "- Set up backup regions for high availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "### üóëÔ∏è Clean up resources\n",
    "\n",
    "When you're finished with the lab, remove all resources to avoid extra charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to delete the resource group and all resources\n",
    "# !az group delete --name {resource_group_name} --yes --no-wait\n",
    "\n",
    "print(f\"To clean up resources, run:\")\n",
    "print(f\"az group delete --name {resource_group_name} --yes\")\n",
    "print(f\"\\nOr use the clean-up-resources notebook for a guided approach.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}